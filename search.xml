<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo博客的安装教程</title>
    <url>/2020/09/02/Hexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><hr>
<p>&emsp;&emsp;博客：正式名称为网络日记，是使用特定的软件，在网络上出版、发表和张贴个人文章的人，或者是一种通常由个人管理、不定期张贴新的文章的网站。<br>&emsp;&emsp;由于最近看了比较多的书和学习视屏后发现很多知识容易遗忘，并且很多技术上的难点当时想通了但是之后就很容易忘记。于是想着建个个人的博客站点来记录平时学到的知识和解决的难题并且也有助于技术的分享。<br>&emsp;&emsp;本教程主要讲述Hexo博客的安装，使用的操作系统为macOS，其他系统也是大同小异。此教程中的博客是部署到GitHub中，请先安装并且配置好GitHub。</p>
<h3 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h3><hr>
<p>先下载<a href="https://nodejs.org/en/">node.js</a>，点击可进入下载网址。官网界面如下：<br><img src= "/img/loading.gif" data-lazy-src="/2020/09/02/Hexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/nodejs.jpeg" alt><br>点击如图所示的永久版本：<br><img src= "/img/loading.gif" data-lazy-src="/2020/09/02/Hexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/nodejs2.jpeg" alt><br>下载完成后对其进行安装。<br>安装好后打开终端，输入如下命令进入root目录：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo su</span><br></pre></td></tr></table></figure><br>输入如下命令测试nodejs有没有安装成功：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm -v</span><br></pre></td></tr></table></figure><br>如果显示了版本好则说明nodejs安装成功。<br>本来是借助npm包管理器来安装，但是国内镜像速度比较慢，先输入如下命令安装淘宝镜像：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -g cnpm --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org</span><br></pre></td></tr></table></figure><br>输入如下命令验证cnpm是否安装成功：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cnpm -v</span><br></pre></td></tr></table></figure><br>如果出现版本号则说明cnpm安装成功。<br>然后用cnpm来安装hexo，命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cnpm install -g hexo-cli</span><br></pre></td></tr></table></figure><br>这样hexo博客就安装完成，输入如下命令验证：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo -v</span><br></pre></td></tr></table></figure><br>若出现版本好则hexo博客安装成功。</p>
<h3 id="搭建Hexo博客"><a href="#搭建Hexo博客" class="headerlink" title="搭建Hexo博客"></a>搭建Hexo博客</h3><hr>
<p>在root目录下建立blog文件夹（名字可自取）命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir blog</span><br></pre></td></tr></table></figure><br>进入blog：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd blog&#x2F;</span><br></pre></td></tr></table></figure><br>在blog文件夹下初始化hexo，命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo hexo init</span><br></pre></td></tr></table></figure><br>初始化完成，主要生成一些框架的内容。<br>启动hexo博客，命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><br>输入命令后电脑会默认在4000端口生成hexo博客，可用浏览器输入localhost:4000来从本地访问hexo博客。至此博客已经搭建完成。</p>
<h3 id="更新hexo博客"><a href="#更新hexo博客" class="headerlink" title="更新hexo博客"></a>更新hexo博客</h3><hr>
<p>当新建文档或者换了新的配置后需要刷新hexo，命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure><br>此命令为清除缓存文件 (db.json) 和已生成的静态文件 (public)。<br>接下来生成静态文件，命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><br>然后再重新启动hexo，命令如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><br>这样就重新在本地更新了hexo博啦。</p>
<h3 id="上传到GitHub"><a href="#上传到GitHub" class="headerlink" title="上传到GitHub"></a>上传到GitHub</h3><hr>
<p>先到GitHub上新建一个项目，项目名字为(github用户名).github.io<br>使用如下命令可以将博客部署到GitHub上：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><br>哈哈，就是这么简单，在浏览器中输入刚刚新建的GitHub项目的网址就能显示你的个人博客啦。（前提是GitHub安装并成功配置）</p>
<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><hr>
<p>关于hexo的使用可进入<a href="https://hexo.io/zh-cn/docs/">hexo使用文档</a>，里面详细记载了hexo的配置和操作命令，这样就能够更改博客的头像和名称等等啦。还有有关给hexo配置主题的问题，我将会在下个博客中详细介绍。</p>
]]></content>
      <categories>
        <category>开发基本技能</category>
        <category>博客</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>Hexo</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title>基于深度学习的激光束识别（绪论）</title>
    <url>/2021/02/25/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%BF%80%E5%85%89%E6%9D%9F%E8%AF%86%E5%88%AB%EF%BC%88%E7%BB%AA%E8%AE%BA%EF%BC%89/</url>
    <content><![CDATA[<h2 id="基于深度学习的激光束识别（绪论）"><a href="#基于深度学习的激光束识别（绪论）" class="headerlink" title="基于深度学习的激光束识别（绪论）"></a>基于深度学习的激光束识别（绪论）</h2><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>这其实是导师安排的一个小项目。为了解决煤矿箕斗倒煤倒不完全无法实时报警的问题，通过实时识别激光束的长短来判断箕斗有没有把煤倒完全。老师们在现场采集了场地的视频，然后通过对视频里的图片用神经网络进行训练从而得出网络参数，最后就能通过实时获取的图片预测出激光束具体的长度和位置。在这里只是讲述了能够把激光束识别出来并与视屏进行合成的大概的效果的实现，因为优化是个繁琐，复杂和漫长的过程，在这里也不一一赘述。在这个项目中我们使用了OpenCV来进行前期和后期的视屏图像处理，同时也使用pytorch框架来进行网络的搭建、训练和预测。</p>
<h4 id="实现的内容"><a href="#实现的内容" class="headerlink" title="实现的内容"></a>实现的内容</h4><ol>
<li><a href="基于深度学习的激光束识别（绪论）.md">基于深度学习的激光束识别(绪论)</a></li>
<li><a href="基于深度学习的激光束识别（数据）.md">基于深度学习的激光束识别(数据)</a></li>
<li><a href="基于深度学习的激光束识别（训练）.md">基于深度学习的激光束识别(数据)</a></li>
<li><a href="基于深度学习的激光束识别（预测）.md">基于深度学习的激光束识别(数据)</a></li>
</ol>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>激光束识别</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>激光束识别</tag>
      </tags>
  </entry>
  <entry>
    <title>基于深度学习的激光束识别（训练）</title>
    <url>/2021/02/25/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%BF%80%E5%85%89%E6%9D%9F%E8%AF%86%E5%88%AB%EF%BC%88%E8%AE%AD%E7%BB%83%EF%BC%89/</url>
    <content><![CDATA[<h2 id="基于深度学习的激光束识别（训练）"><a href="#基于深度学习的激光束识别（训练）" class="headerlink" title="基于深度学习的激光束识别（训练）"></a>基于深度学习的激光束识别（训练）</h2><p>当我们完成了数据的准备工作后，接下来就是要训练了。训练之前，首先要设计好训练的网络，然后设计合适的损失函数，最后就是开始训练了。</p>
<h4 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h4><p>网络设计目前还没有啥可以依据的理论，大部分全靠经验和不断尝试来设计网络。在这里，我取输入的长宽都是1000的主要目的是为了后面进行归一化的时候能够整除，这样就减少了截断误差。网络的层数也是从小到大逐渐尝试慢慢得出最好的结果。然后卷积盒的大小我是从大到小来取的，个人感觉这样能够从大到小取到特征值。网络设计的代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		super(net, self).__init__()</span><br><span class="line"></span><br><span class="line">		self.Conv_layers1 = nn.Sequential(</span><br><span class="line">				nn.Conv2d(<span class="number">3</span>, <span class="number">128</span>, <span class="number">10</span>, <span class="number">2</span>),</span><br><span class="line">				nn.LeakyReLU(),</span><br><span class="line">				nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">			)</span><br><span class="line"></span><br><span class="line">		self.Conv_layers2 = nn.Sequential(</span><br><span class="line">				nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">6</span>, <span class="number">2</span>),</span><br><span class="line">				nn.LeakyReLU(),</span><br><span class="line">				nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">			)</span><br><span class="line"></span><br><span class="line">		self.Conv_layers3 = nn.Sequential(</span><br><span class="line">				nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">				nn.LeakyReLU(),</span><br><span class="line">				nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">			)</span><br><span class="line"></span><br><span class="line">		self.Conn_layers1 = nn.Sequential(</span><br><span class="line">				nn.Linear(<span class="number">15</span> * <span class="number">15</span> * <span class="number">512</span>, <span class="number">2000</span>),</span><br><span class="line">				nn.LeakyReLU()</span><br><span class="line">			)</span><br><span class="line"></span><br><span class="line">		self.Conn_layers2 = nn.Sequential(</span><br><span class="line">				nn.Linear(<span class="number">2000</span>, <span class="number">5</span>),</span><br><span class="line">				nn.Sigmoid()</span><br><span class="line">			)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input</span>):</span></span><br><span class="line">		input = self.Conv_layers1(input)</span><br><span class="line">		input = self.Conv_layers2(input)</span><br><span class="line">		input = self.Conv_layers3(input)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将数据的维度改为全连接层的输入维度</span></span><br><span class="line">		input = input.view(input.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">		input = self.Conn_layers1(input)</span><br><span class="line">		input = self.Conn_layers2(input)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将重置输出数据形状</span></span><br><span class="line">		output = input.reshape(<span class="number">-1</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>在这里我采用了最经典的预测值与实际值的差的绝对值来计算损失。当然，后期优化可以用性能更好的损失函数。损失函数如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">predict, labels</span>):</span></span><br><span class="line">	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	功能：计算损失值</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	——predict：net的预测结果(batchsize,5)，其中predict为tensor</span></span><br><span class="line"><span class="string">	——labels：样本数据(batchsize,5)，其中labels为tensor</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	——loss：平均损失</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">	<span class="comment"># 设置初始损失值</span></span><br><span class="line">	loss = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取batchsize</span></span><br><span class="line">	batch = labels.size()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 计算每个批次的总损失</span></span><br><span class="line">	<span class="keyword">for</span> batch_size <span class="keyword">in</span> range(batch):</span><br><span class="line">		<span class="keyword">if</span> labels[batch_size, <span class="number">4</span>] == <span class="number">1</span>:</span><br><span class="line">			loss = loss + torch.sum((predict[batch_size, <span class="number">0</span>:<span class="number">5</span>] - labels[batch_size, <span class="number">0</span>:<span class="number">5</span>]) ** <span class="number">2</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			loss = loss + <span class="number">5</span> * torch.sum((predict[batch_size, <span class="number">4</span>] - labels[batch_size, <span class="number">4</span>]) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 求平均损失</span></span><br><span class="line">	loss = loss / batch</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></p>
<h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><p>终于要开始训练了，前面已经整合好了需要训练的一些文档和数据，所以直接训练就行。训练程序如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> dataset <span class="keyword">as</span> D</span><br><span class="line"><span class="keyword">import</span> net <span class="keyword">as</span> N</span><br><span class="line"><span class="keyword">import</span> loss_function <span class="keyword">as</span> LF</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置迭代次数</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置批量的大小</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置学习率</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化数据对象</span></span><br><span class="line">train_data = D.dataset(is_train = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = DataLoader(train_data, batch_size = batch_size, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个网络对象</span></span><br><span class="line">net = N.net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化函数为随机梯度下降</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr = lr, momentum = <span class="number">0.9</span>, weight_decay = <span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行迭代运算</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epoch):</span><br><span class="line">	<span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> enumerate(data):</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将输入值设置为浮点输入</span></span><br><span class="line">		inputs = inputs.float()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 获取预测数据</span></span><br><span class="line">		predict = net(inputs)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 计算损失</span></span><br><span class="line">		loss = LF.loss_function(predict, labels)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将梯度归零</span></span><br><span class="line">		optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 进行反向传播运算</span></span><br><span class="line">		loss.backward()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 更新参数</span></span><br><span class="line">		optimizer.step()</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 每迭代两次存一次训练好的参数模型</span></span><br><span class="line">	<span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 存参数模型</span></span><br><span class="line">		torch.save(net, <span class="string">&quot;weight1/weight&quot;</span> + str(e + <span class="number">1</span>) + <span class="string">&quot;.pkl&quot;</span>)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>激光束识别</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>激光束识别</tag>
      </tags>
  </entry>
  <entry>
    <title>基于深度学习的激光束识别（数据）</title>
    <url>/2021/02/25/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%BF%80%E5%85%89%E6%9D%9F%E8%AF%86%E5%88%AB%EF%BC%88%E6%95%B0%E6%8D%AE%EF%BC%89/</url>
    <content><![CDATA[<h2 id="基于深度学习的激光束识别（数据）"><a href="#基于深度学习的激光束识别（数据）" class="headerlink" title="基于深度学习的激光束识别（数据）"></a>基于深度学习的激光束识别（数据）</h2><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>在深度学习的项目中，可以说数据是十分重要的。因为如果数据有错误或者数据不合适的话，那后面所设计的网络和所调节的参数可能都要重新更改过，这样就会严重影响项目完成的效率。在这里数据主要分为两部分：一是数据准备，二是在pytorch中进行数据加载。</p>
<h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>数据来源主要是导师从现场拍摄的现场视频，这应该是最原始的数据了。以下为最原始的MP4数据：<br><img src= "/img/loading.gif" data-lazy-src="/2021/02/25/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%BF%80%E5%85%89%E6%9D%9F%E8%AF%86%E5%88%AB%EF%BC%88%E6%95%B0%E6%8D%AE%EF%BC%89/laser_mp4.jpeg" alt><br>首先我们做的是用OpenCV将视屏拆成一帧一帧的图片，具体代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要提取视频的文件名，隐藏后缀</span></span><br><span class="line">source_file_name = <span class="string">&#x27;laser&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这里把后缀接上</span></span><br><span class="line">video_path = os.path.join(<span class="string">&quot;video/&quot;</span>, source_file_name + <span class="string">&#x27;.mp4&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取视频的频率，每１帧提取一个</span></span><br><span class="line">frame_frequency = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图片到当前目录vedio文件夹下</span></span><br><span class="line">out_put_directory_name = <span class="string">&#x27;Image2/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果文件目录不存在则创建目录</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(out_put_directory_name):</span><br><span class="line">    os.makedirs(out_put_directory_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从路径中获取视屏并放入变量中</span></span><br><span class="line">camera = cv2.VideoCapture(video_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置初始num的值</span></span><br><span class="line">num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始从视屏中提取图片</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    num = num + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取视屏中的图片</span></span><br><span class="line">    res, image = camera.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果视屏图片全部读完了，提取图片结束</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> res:</span><br><span class="line">        print(<span class="string">&#x27;not res , not image&#x27;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将图片以num值为名写入指定路径的文档中并打出图片名称</span></span><br><span class="line">    cv2.imwrite(out_put_directory_name + str(int(num)) + <span class="string">&#x27;.jpg&#x27;</span>, image)</span><br><span class="line">    print(out_put_directory_name + str(int(num)) + <span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图片提取结束</span></span><br><span class="line">print(<span class="string">&#x27;图片提取结束&#x27;</span>)</span><br><span class="line">camera.release()</span><br></pre></td></tr></table></figure><br>在将视屏分解为各个图片后，接下来我们就得在图片上标数据了。至于怎么标数据这点我们开始讨论了很久。因为要识别光束线的长度，那么计算机就得把完整的光束都识别出来，这样就能够计算出光束的长度了。要识别整个光束就意味着要找到光束的特征点，很明显光束有两个特征点就是一头一尾。为什么说找到一头一尾就能够知道这是光束呢？其实神经网络认识到一个物体可以用人的方式进行思考，人一眼就能看出光束的头和尾是因为头和尾的像素特征和周围的环境有着比较明显的差别，所以计算机也能通过这些像素的特征差别来进行寻找和区分光束和背景。所以我们标注了光束的头和尾的像素的坐标点。坐标点的方法是一张图片一个txt文档然后以[w_top h_top w_bottom h_bottom have_or_not]的格式来表示激光束的标注的坐标值。例如[302 396 315 915 1]表示激光束顶部的横坐标是302纵坐标是396，底部的横坐标是315纵坐标是915，然后1表示图片中有激光束的存在。<br>标注完坐标点后我们对原始图片进行裁剪。因为我们最初设计的网络是用<script type="math/tex">1000*1000</script>的RGB图来作为输入数据的（训练模块会解释为什么用<script type="math/tex">1000*1000</script>大小的图片），然后我们得用OpenCV将图片裁剪成<script type="math/tex">1000*1000</script>大小的图片。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tailor</span>(<span class="params">item</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	功能：将指定图片名的图片按照对应尺寸进行裁剪并存储</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">		item：需要裁剪的图片名（不带后缀）</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 读取图片</span></span><br><span class="line">	img = cv2.imread(<span class="string">&quot;Image/&quot;</span> + item + <span class="string">&quot;.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将高h和宽w按照指定img[h,w]像素比例进行裁剪</span></span><br><span class="line">	tailor_image = img[<span class="number">300</span>:<span class="number">1300</span>, <span class="number">0</span>:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将裁剪后的图片存入文件夹中</span></span><br><span class="line">	cv2.imwrite(<span class="string">&quot;Image3/&quot;</span> + item + <span class="string">&quot;.jpg&quot;</span>, tailor_image)</span><br></pre></td></tr></table></figure>
<p>以上为将图片按照对应像素点比例进行裁剪的函数，通过以上函数我们能够将每张要训练的图片裁剪成<script type="math/tex">1000*1000</script>像素的RGB图片。<br>接下来就要进行图片标注数据的解析与归一化了。因为数据都是整数，所以要将数据进行归一化。归一化操作为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal</span>(<span class="params">x1, y1, x2, y2, w, h</span>):</span></span><br><span class="line">	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	功能：将标注坐标(x,y)进行归一化</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	——x1：上顶点标注的横坐标</span></span><br><span class="line"><span class="string">	——y1：上顶点标注的纵坐标</span></span><br><span class="line"><span class="string">	——x2：下顶点标注的横坐标</span></span><br><span class="line"><span class="string">	——y2：下顶点标注的纵坐标</span></span><br><span class="line"><span class="string">	——w：原图片的宽</span></span><br><span class="line"><span class="string">	——h：原图片的高</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	——x1：上顶点归一化后的横坐标值</span></span><br><span class="line"><span class="string">	——y1：上顶点归一化后的纵坐标值</span></span><br><span class="line"><span class="string">	——x2：下顶点归一化后的横坐标值</span></span><br><span class="line"><span class="string">	——y2：下顶点归一化后的纵坐标值</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将横纵坐标转换成浮点数</span></span><br><span class="line">	x1 = x1 / <span class="number">1.0</span></span><br><span class="line">	y1 = y1 / <span class="number">1.0</span></span><br><span class="line">	x2 = x2 / <span class="number">1.0</span></span><br><span class="line">	y2 = y2 / <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将横纵坐标进行归一化</span></span><br><span class="line">	x1 = x1 / w</span><br><span class="line">	y1 = y1 / h</span><br><span class="line">	x2 = x2 / w</span><br><span class="line">	y2 = y2 / h</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> (x1, y1, x2, y2)</span><br></pre></td></tr></table></figure><br>知道怎么归一化了就得将数据从txt文档中提取出来然后进行归一化最后放入到新的txt文档中。具体操作如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_annotation</span>(<span class="params">image_id</span>):</span></span><br><span class="line">	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	功能：将Annotation中的txt文档中的坐标进行归一化然后</span></span><br><span class="line"><span class="string">	存入labels中。</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	——image_id：Annotations中的.txt文档的文件名例如&#x27;1.txt&#x27;</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">	<span class="keyword">with</span> open(<span class="string">&#x27;Annotations/%s&#x27;</span> % (image_id)) <span class="keyword">as</span> in_file:</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将文件名以&#x27;.&#x27;隔开取前面数字字符串放入image_id中</span></span><br><span class="line">		image_id = image_id.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将.txt文档中的内容以每个整行数值的格式方式列表中</span></span><br><span class="line">		line = [x.split() <span class="keyword">for</span> x <span class="keyword">in</span> in_file]</span><br><span class="line">		pixel = [int(x) <span class="keyword">for</span> y <span class="keyword">in</span> line <span class="keyword">for</span> x <span class="keyword">in</span> y]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 读入图片数据并将pixel内的坐标点放入归一化函数进行归一化</span></span><br><span class="line">		img = cv2.imread(<span class="string">&#x27;Image/&#x27;</span> + image_id + <span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line">		normal = normal(pixel[<span class="number">0</span>], pixel[<span class="number">1</span>] - <span class="number">300</span>, pixel[<span class="number">2</span>],</span><br><span class="line">					pixel[<span class="number">3</span>] - <span class="number">300</span>, img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将归一化后的数值放入pixel中</span></span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">			pixel[i] = normal[i]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将归一化后的pixel内的数值存入labels中的.txt文档内</span></span><br><span class="line">		<span class="keyword">with</span> open(<span class="string">&#x27;labels/%s.txt&#x27;</span> % (image_id), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> out_file:</span><br><span class="line">			out_file.write(<span class="string">&quot; &quot;</span>.join([str(a) <span class="keyword">for</span> a <span class="keyword">in</span> pixel]) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 打印出文件名id和归一化后的坐标</span></span><br><span class="line">	print(<span class="string">&#x27;image_id=&#123;&#125;, pixel=&#123;&#125;&#x27;</span>.format(image_id, pixel))</span><br></pre></td></tr></table></figure></p>
<h4 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h4><p>经过我们数据准备阶段，最重要的归一化后的标注数据已经放在了labels文档当中了，接下来就是新建一个dataset类了。在pytorch中会自动调用dataset类来进行数据的加载。此类的代码为：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">dataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train = True</span>):</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 设置存储着文件名的列表</span></span><br><span class="line">		self.file_names = []</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 如果是要进行训练读取训练数据文档，否则读取验证数据文档</span></span><br><span class="line">		<span class="keyword">if</span> is_train:</span><br><span class="line">			<span class="comment"># 打开训练文档</span></span><br><span class="line">			<span class="keyword">with</span> open(<span class="string">&quot;ImageSets/Main/train.txt&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">				<span class="comment"># 将训练数据的文档名存储到文件列表中,.strip()去除字符串前后的空格或换行符</span></span><br><span class="line">				self.file_names = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> f]</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			<span class="comment"># 打开验证数据文档</span></span><br><span class="line">			<span class="keyword">with</span> open(<span class="string">&quot;ImageSets/Main/val.txt&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">				<span class="comment"># 将验证数据的文档名存储到文件列表中</span></span><br><span class="line">				self.file_names = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> f]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 图片存储路径和label数据文档存储数据路径</span></span><br><span class="line">		self.img_path = <span class="string">&quot;Image/&quot;</span></span><br><span class="line">		self.label_path = <span class="string">&quot;labels/&quot;</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">		功能：返回文件名的个数</span></span><br><span class="line"><span class="string">		&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 返回文件名的个数</span></span><br><span class="line">		<span class="keyword">return</span> len(self.file_names)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, item</span>):</span></span><br><span class="line">		<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">		功能：对图像数据进行规范化处理</span></span><br><span class="line"><span class="string">		参数：</span></span><br><span class="line"><span class="string">			picture_index：图片在文件名当中的索引</span></span><br><span class="line"><span class="string">		返回值：</span></span><br><span class="line"><span class="string">			img：经过维度转换后的图片的数值</span></span><br><span class="line"><span class="string">			labels：转换成numpy数组后的label数据</span></span><br><span class="line"><span class="string">		&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 读取需要图像处理和数据转换的图像</span></span><br><span class="line">		img = cv2.imread(self.img_path + self.file_names[item] + <span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将img数据数值进行维度转换原先(h,w,c)，转换后(c,w,h)</span></span><br><span class="line">		img = img.transpose(<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将像素值进行归一化处理</span></span><br><span class="line">		img = img / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">with</span> open(self.label_path+self.file_names[item] + <span class="string">&quot;.txt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">			<span class="comment"># 将.txt文档中的内容放入列表中</span></span><br><span class="line">			line = [x.split() <span class="keyword">for</span> x <span class="keyword">in</span> f]</span><br><span class="line">			label = [float(x) <span class="keyword">for</span> y <span class="keyword">in</span> line <span class="keyword">for</span> x <span class="keyword">in</span> y]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 新建一个值都是零的大小为5的numpy数组</span></span><br><span class="line">		labels = np.zeros((<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 将标注的值都放入numpy数组中</span></span><br><span class="line">		labels[<span class="number">0</span>:<span class="number">5</span>] = np.array([label[<span class="number">0</span>], label[<span class="number">1</span>], label[<span class="number">2</span>], label[<span class="number">3</span>], label[<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> img, labels</span><br></pre></td></tr></table></figure><br>至此，数据准备阶段已全部完成。</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>激光束识别</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>激光束识别</tag>
      </tags>
  </entry>
  <entry>
    <title>基于深度学习的激光束识别（预测）</title>
    <url>/2021/02/25/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%BF%80%E5%85%89%E6%9D%9F%E8%AF%86%E5%88%AB%EF%BC%88%E9%A2%84%E6%B5%8B%EF%BC%89/</url>
    <content><![CDATA[<h2 id="基于深度学习的激光束识别（预测）"><a href="#基于深度学习的激光束识别（预测）" class="headerlink" title="基于深度学习的激光束识别（预测）"></a>基于深度学习的激光束识别（预测）</h2><p>由于预测结果主要是激光上下定点的坐标值，所以在这里我将预测结果表示的两个定点用直线连起来就模拟出预测的激光束。先将原来的视屏拆成了一张一张的图片，然后对一张一张的图片进行预测，最后再把每张图片连起来拼成处理好的视屏保存起来。具体实现代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_video</span>(<span class="params">video_path, result_video_path</span>):</span></span><br><span class="line">	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	功能：将预测结果转换成视频并保存起来</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">		video_path：需要处理的原始视频路径</span></span><br><span class="line"><span class="string">		result_video_path：处理后的视屏路径</span></span><br><span class="line"><span class="string">	&#x27;&#x27;&#x27;</span></span><br><span class="line">	batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 初始化文件名</span></span><br><span class="line">	video = <span class="string">&quot;laser.mp4&quot;</span></span><br><span class="line">	result_video = <span class="string">&quot;result_laser.mp4&quot;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 加载权重</span></span><br><span class="line">	net = torch.load(<span class="string">&quot;weight/weight10.pkl&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 读取视屏</span></span><br><span class="line">	cap = cv2.VideoCapture(video_path + video)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取视屏帧率</span></span><br><span class="line">	fps_video = cap.get(cv2.CAP_PROP_FPS)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 设置写入视屏的编码格式</span></span><br><span class="line">	fourcc = cv2.VideoWriter_fourcc(*<span class="string">&quot;mp4v&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取视屏宽度和高度</span></span><br><span class="line">	frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))</span><br><span class="line">	frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 设置写视屏的对象</span></span><br><span class="line">	videoWriter = cv2.VideoWriter(result_video_path + result_video, fourcc, fps_video, (frame_width, frame_height))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> (cap.isOpened()):</span><br><span class="line">		ret, frame = cap.read()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 如果视屏没有结束</span></span><br><span class="line">		<span class="keyword">if</span> ret == <span class="literal">True</span>:</span><br><span class="line">			<span class="comment"># 将图片进行剪切</span></span><br><span class="line">			tailor_frame = frame[<span class="number">300</span>:<span class="number">1300</span>, <span class="number">0</span>:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 初始化输入网络的numpy数组</span></span><br><span class="line">			inputs = np.zeros((<span class="number">1</span>, <span class="number">3</span>, <span class="number">1000</span>, <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 将每一帧的图片像素值赋值给输入矩阵</span></span><br><span class="line">			<span class="keyword">for</span> wide <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">				<span class="keyword">for</span> high <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">					<span class="keyword">for</span> channel <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">						inputs[<span class="number">0</span>, channel, wide, high] = tailor_frame[high, wide, channel] / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">			<span class="comment"># 进行输入数据类型转换</span></span><br><span class="line">			inputs = torch.from_numpy(inputs)</span><br><span class="line">			inputs = inputs.float()</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 得到预测数据</span></span><br><span class="line">			predict = net(inputs)</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 如果为箕斗上来的图片时对图片进行处理</span></span><br><span class="line">			<span class="keyword">if</span> float(predict[<span class="number">0</span>, <span class="number">4</span>]) &gt; <span class="number">0.6</span>:</span><br><span class="line">				<span class="comment"># 初始化一个tensor向量</span></span><br><span class="line">				predict_conversion = torch.zeros((<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">				<span class="comment"># 将预测值中的坐标赋值给tensor向量</span></span><br><span class="line">				<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">					predict_conversion[i] = predict[<span class="number">0</span>, i]</span><br><span class="line"></span><br><span class="line">				<span class="comment"># 将预测值进行坐标换算，横坐标不变纵坐标加三百</span></span><br><span class="line">				predict_conversion[<span class="number">0</span>] = (predict_conversion[<span class="number">0</span>] * <span class="number">1000</span>)</span><br><span class="line">				predict_conversion[<span class="number">2</span>] = (predict_conversion[<span class="number">2</span>] * <span class="number">1000</span>)</span><br><span class="line">				predict_conversion[<span class="number">1</span>] = (predict_conversion[<span class="number">1</span>] * <span class="number">1000</span> + <span class="number">300</span>)</span><br><span class="line">				predict_conversion[<span class="number">3</span>] = (predict_conversion[<span class="number">3</span>] * <span class="number">1000</span> + <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">				<span class="comment"># 绘制两个坐标点</span></span><br><span class="line">				pt1 = (int(predict_conversion[<span class="number">0</span>]), int(predict_conversion[<span class="number">1</span>]))</span><br><span class="line">				pt2 = (int(predict_conversion[<span class="number">2</span>]), int(predict_conversion[<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">				<span class="comment"># 在每一帧图片上画出预测的绿线</span></span><br><span class="line">				cv2.line(frame, pt1, pt2, (<span class="number">0</span>, <span class="number">250</span>, <span class="number">0</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 显示视频</span></span><br><span class="line">			cv2.imshow(<span class="string">&#x27;Frame&#x27;</span>,frame)</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 刷新视频</span></span><br><span class="line">			cv2.waitKey(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 按q键退出</span></span><br><span class="line">			<span class="keyword">if</span> cv2.waitKey(<span class="number">25</span>) &amp; <span class="number">0xFF</span> == ord(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">				<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">			<span class="comment"># 将图片写入视屏</span></span><br><span class="line">			videoWriter.write(frame)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			<span class="comment"># 写入视屏结束</span></span><br><span class="line">			videoWriter.release()</span><br><span class="line">			<span class="keyword">break</span></span><br></pre></td></tr></table></figure><br>最后整个项目初步的功能已经实现了。</p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>激光束识别</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>激光束识别</tag>
      </tags>
  </entry>
  <entry>
    <title>手写数字识别源码分析</title>
    <url>/2021/02/25/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>​        手写数字识别是作为学习神经网络和深度学习最基础的实践项目。这篇博文里的代码均来自《神经网络与深度学习》一书。在network.py中虽然只有短短的七十多行代码，但代码压缩紧密，看起来比较费劲。在刚开始学习神经网络的时候理解了代码，但过了一个学期回过头再来看的时候发现还得重新理一遍。于是将其源码写上自己的注释和理解以便于以后能够快速回顾和后面学习者能够更轻松学习此源码。<br>​        此注释代码为network1中的最基础的神经网络，采用了极少的优化步骤。在代码注释中我比较喜欢假设一组最简单的数据代入其中来方便理解其中复杂的维度转换和矩阵点乘的运算。在此代码中，假设有三层网络，每层的神经元个数分别为[2,3,1]，训练集和测试集数据都为10组，每5组为一批，以此假设为注释来理解代码。</p>
<h4 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, sizes</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        功能：重写初始化函数，并初始化属性</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            sizes：存放网络各层神经元参数的列表，例如sizes=[2,3,1]</span></span><br><span class="line"><span class="string">                   说明有三层网络，每层网络神经元个数分别为2个3个1个</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化网络层数</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化网络各参数的列表</span></span><br><span class="line">        self.sizes = sizes</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化偏执，初始值为使用高斯分布均值0，方差1的分布。</span></span><br><span class="line"><span class="string">        测试程序：</span></span><br><span class="line"><span class="string">        import numpy</span></span><br><span class="line"><span class="string">        sizes = [2, 3, 1]</span></span><br><span class="line"><span class="string">        biases = [numpy.random.randn(y, 1) for y in sizes[1:]]</span></span><br><span class="line"><span class="string">        biases的结果：</span></span><br><span class="line"><span class="string">        [array([[-1.39730573],</span></span><br><span class="line"><span class="string">                [-0.84395433],</span></span><br><span class="line"><span class="string">                [ 0.66160829]]), array([[1.03552743]])]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化权重，初始值为使用高斯分布均值0，方差1的分布。</span></span><br><span class="line"><span class="string">        设sizes = [2, 3, 1]</span></span><br><span class="line"><span class="string">        则sizes[:-1] = [2, 3]</span></span><br><span class="line"><span class="string">        sizes[1:] = [3, 1]</span></span><br><span class="line"><span class="string">        zip(sizes[:-1], sizes[1:]) = [(2, 3), (3, 1)]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span>(<span class="params">self, a</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        功能：进行前向传播操作</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            a：形状为(n,1)的输入值</span></span><br><span class="line"><span class="string">        返回值：</span></span><br><span class="line"><span class="string">            a：进行前向传播后的输出值</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播计算</span></span><br><span class="line"><span class="string">        设sizes = [2, 3, 1]</span></span><br><span class="line"><span class="string">        biases在1~2和2~3层之间的行列数分别为(3,1)和(1,1)</span></span><br><span class="line"><span class="string">        weights在1~2和2~3层之间的行列数分别为(3,2)和(1,3)</span></span><br><span class="line"><span class="string">        因为是三层网络，所以进行两次循环，分别为：</span></span><br><span class="line"><span class="string">        第二层输出的a=w*a+b=(3,2)dot(2,1)+(3,1)=(3,1)</span></span><br><span class="line"><span class="string">        第三层输出的a=w*a+b=(1,3)dot(3,1)+(1,1)=(1,1)</span></span><br><span class="line"><span class="string">        由上述假设所得到的a虽然只有一个值但是是二维的</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a) + b)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            test_data = None</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        功能：进行随机梯度下降，训练或者测试数据集并显示结果</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            training_data：例如(([[0x1,0x2]T],[[0y]]),......,([[9x1,9x2]T],[[9y]]))的训练集</span></span><br><span class="line"><span class="string">            test_data：例如(([[0x1,0x2]T],[[0y]]),......,([[9x1,9x2]T],[[9y]]))的测试集</span></span><br><span class="line"><span class="string">            epochs：迭代次数</span></span><br><span class="line"><span class="string">            mini_batch_size：一个批次的训练数量</span></span><br><span class="line"><span class="string">            eta：学习率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将training_data的元组转换为列表并获取列表长度</span></span><br><span class="line"><span class="string">        假设有10组训练数据，那么training_data列表化后的值为：</span></span><br><span class="line"><span class="string">        [([[0x1,0x2]T],[[0y]]),......,([[9x1,9x2]T],[[9y]])]</span></span><br><span class="line"><span class="string">        n的值为10</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        training_data = list(training_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果为测试模式的话，同训练数据所示</span></span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            test_data = list(test_data)</span><br><span class="line">            n_test = len(test_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行迭代，每迭代一次输出一次结果</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(epochs):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将训练数据打乱（每迭代一次打乱一次）</span></span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line"></span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            将训练数据转换为小批量列表数据</span></span><br><span class="line"><span class="string">            假设有10组训练数据，mini_batch_size=5则：</span></span><br><span class="line"><span class="string">            training_data=[([[0x1,0x2]T],[[0y]]),........,([[9x1,9x2]T],[[9y]])]</span></span><br><span class="line"><span class="string">            进行此操作后：</span></span><br><span class="line"><span class="string">            mini_batches=[[([[0x1,0x2]T],[[0y]]),...,([[4x1,4x2]T],[[4y]])],</span></span><br><span class="line"><span class="string">                          [([[5x1,5x2]T],[[5y]]),...,([[9x1,9x2]T],[[9y]])]]</span></span><br><span class="line"><span class="string">            range(0, n, mini_batch_size)]这句话的意思就是在[0,n)中从</span></span><br><span class="line"><span class="string">            0开始每隔mini_batch_size个数取一个数，所以最后取得的k为0，5</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k + mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line"></span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            按照上述的假设我们可以得到：</span></span><br><span class="line"><span class="string">            第一次：mini_batch=[([[0x1,0x2]T],[[0y]]),...,([[4x1,4x2]T],[[4y]])]</span></span><br><span class="line"><span class="string">            第二次：mini_batch=[([[5x1,5x2]T],[[5y]]),...,([[9x1,9x2]T],[[9y]])]</span></span><br><span class="line"><span class="string">            每循环一次便更新一次w和b</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果是测试数据则输出测试结果，反之输出训练结果</span></span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                print(<span class="string">&quot;Epoch &#123;&#125; : &#123;&#125; / &#123;&#125;&quot;</span>.format(j,</span><br><span class="line">                    self.evaluate(test_data), n_test));</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">&quot;Epoch &#123;&#125; complete&quot;</span>.format(j))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        功能：更新一个批次数据后的权重和偏执</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            mini_batch：一个批次的数据集</span></span><br><span class="line"><span class="string">            eta：学习速率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将存放b和w的梯度的列表清零</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对一个批次的数据进行反向传播，将所得梯度进行一个批次的求和</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb + dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b,</span><br><span class="line">                delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw + dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w,</span><br><span class="line">                delta_nabla_w)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据公式w-eta*(sum(dw)/m)和b-eta*(sum(db)/m)更新参数</span></span><br><span class="line">        self.weights = [w-(eta / len(mini_batch)) * nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b - (eta / len(mini_batch)) * nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        功能：进行反向传播</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            x：输入值列表</span></span><br><span class="line"><span class="string">            y：准确值列表</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置要更新的b和w的相应列表并按照b和w的相应形状用零填充进行初始化</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># activation存放初始化的输入值x，只计算第一次</span></span><br><span class="line">        activation = x</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 列表存储所有激活，一层一层</span></span><br><span class="line">        activations = [x]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 列表存储所有激活，一层一层</span></span><br><span class="line">        zs = []</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        此步为前向传播操作并将相关数据进行存储</span></span><br><span class="line"><span class="string">        假设sizes=[2,3,1]，此步操作后各数据行列值为：</span></span><br><span class="line"><span class="string">        zs=[(3,1),(1,1)]</span></span><br><span class="line"><span class="string">        activations=[(2,1),(3,1),(1,1)]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation) + b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播的开始，从后开始获取第一个db值，delta=(1,1)</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将db值传入db的更新列表中</span></span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将dw值传入dw的更新列表中，nabla_w[-1]=(1,3)</span></span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        此操作为从后向前获取新的db和dw并放入其列表中</span></span><br><span class="line"><span class="string">        若假设sizes=[2,3,1]，则此循环进行一次，得到最终行列为：</span></span><br><span class="line"><span class="string">        sp=(3,1)</span></span><br><span class="line"><span class="string">        nabla_b=[(3,1),(1,1)]</span></span><br><span class="line"><span class="string">        nabla_w=[(3,2),(1,3)]</span></span><br><span class="line"><span class="string">        最终返回的nabla_b和nabla_w与self.biases和self.weights形状相同</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l + <span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l - <span class="number">1</span>].transpose())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self, test_data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        功能：对测试数据进行测试</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            test_data：测试数据集</span></span><br><span class="line"><span class="string">        返回值：返回正确的个数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># argmax返回列表中值最大的索引，相当于概率最大的值的索引</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若预测结果等于真实结果将值记为1，并计算总和返回</span></span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span>(<span class="params">self, output_activations, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        功能：计算损失</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            output_activations：预测值</span></span><br><span class="line"><span class="string">            y：实际值</span></span><br><span class="line"><span class="string">        返回值：返回损失</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations - y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    功能：实现sigmoid函数</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        z：w*a+b的值的列表</span></span><br><span class="line"><span class="string">    返回值：返回z的sigmoid函数值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    功能：计算sigmoid函数的导数</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        z：w*a+b的值的列表</span></span><br><span class="line"><span class="string">    返回值：返回z的sigmoid函数的导数值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z) * (<span class="number">1</span> - sigmoid(z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    功能：打开压缩包并获取数据集。</span></span><br><span class="line"><span class="string">    训练集中有50000个组数据，验证集和测试集中分别有10000组数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打开压缩文件</span></span><br><span class="line">    f = gzip.open(<span class="string">&#x27;mnist.pkl.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从文件中下载数据集存入变量中</span></span><br><span class="line">    training_data, validation_data, test_data = pickle.load(f, encoding=<span class="string">&quot;latin1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭文件</span></span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回各类数据集</span></span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_wrapper</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    功能：将数据集整理为相应格式的数据集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从压缩文档中获取数据集</span></span><br><span class="line">    tr_d, va_d, te_d = load_data()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将训练集打包成行列数为((784,1),(10,1))的元组</span></span><br><span class="line">    training_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> tr_d[<span class="number">0</span>]]</span><br><span class="line">    training_results = [vectorized_result(y) <span class="keyword">for</span> y <span class="keyword">in</span> tr_d[<span class="number">1</span>]]</span><br><span class="line">    training_data = zip(training_inputs, training_results)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将验证集打包成行列数为((784,1),1)的元组</span></span><br><span class="line">    validation_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> va_d[<span class="number">0</span>]]</span><br><span class="line">    validation_data = zip(validation_inputs, va_d[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将测试集打包成行列数为((784,1),1)的元组</span></span><br><span class="line">    test_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> te_d[<span class="number">0</span>]]</span><br><span class="line">    test_data = zip(test_inputs, te_d[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回各类数据集</span></span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorized_result</span>(<span class="params">j</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    功能：将正确值转换为索引号为对应值为1其余值为0的列表</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        j：正确值索引</span></span><br><span class="line"><span class="string">    返回值：</span></span><br><span class="line"><span class="string">        e：转换后的列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 加载数据集</span></span><br><span class="line">    training_data, validation_data, test_data = </span><br><span class="line">        load_data_wrapper()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对网络进行初始化</span></span><br><span class="line">    net = Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机梯度下降显示测试结果</span></span><br><span class="line">    net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>人工智能</category>
        <category>手写数字识别</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
        <tag>手写数字识别</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络基础1</title>
    <url>/2020/10/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%801/</url>
    <content><![CDATA[<h1 id="神经网络基础1"><a href="#神经网络基础1" class="headerlink" title="神经网络基础1"></a>神经网络基础1</h1><h3 id="关于iPython笔记本"><a href="#关于iPython笔记本" class="headerlink" title="关于iPython笔记本"></a>关于iPython笔记本</h3><hr>
<p>iPython Notebook（是jupyter notebook的前称）是嵌入在网页中的交互式编码环境。运行iPython笔记本的方法为在终端（Mac版）输入命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><br>Jupyter notebook的默认端口号为8888，因此在浏览器中输入localhost:8888就能访问。</p>
<h3 id="使用numpy构建基本函数"><a href="#使用numpy构建基本函数" class="headerlink" title="使用numpy构建基本函数"></a>使用numpy构建基本函数</h3><hr>
<h4 id="sigmoid-function和np-exp（）"><a href="#sigmoid-function和np-exp（）" class="headerlink" title="sigmoid function和np.exp（）"></a>sigmoid function和np.exp（）</h4><p>sigmoid函数是激活函数的一种，其表达式为：$sigmoid(x) = \frac{1}{1+e^{-x}}$在这里主要对比了np.exp()和math.exp()之间的差别，其目的是为了说明math.exp()主要对单个的数值进行运算，而np.exp()则是对矩阵进行相关的运算。</p>
<h4 id="Sigmoid-gradient"><a href="#Sigmoid-gradient" class="headerlink" title="Sigmoid gradient"></a>Sigmoid gradient</h4><p>sigmoid函数梯度计算公式为：<script type="math/tex">sigmoid\_derivative(x) = \sigma'(x) = \sigma(x) (1 - \sigma(x))</script>，求梯度的目的主要是要知道曲线在某个方向倾斜程度。在这里将s设为sigmoid(x)，然后再计算$\sigma’(x) = s(1-s)$即可。实现代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	功能：求在sigmoid函数中x的梯度</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	x -- 一个标量或者numpy数组</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	ds -- 计算出的梯度</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	s = sigmoid(x) </span><br><span class="line">	ds = s * (<span class="number">1</span> - s)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure></p>
<h4 id="重塑数组"><a href="#重塑数组" class="headerlink" title="重塑数组"></a>重塑数组</h4><p>np.reshape()用于将X重塑为其他尺寸。在计算机科学中，图像由shape为$(length, height, depth = 3)$的3D数组表示。但是，当你读取图像作为算法的输入时，会将其转换为维度为$(length<em>height</em>3, 1)$的向量。换句话说，将3D阵列“展开”或重塑为1D向量。</p>
<p><strong>练习</strong>：实现<code>image2vector()</code> ,该输入采用维度为(length, height, 3)的输入，并返回维度为(length*height*3, 1)的向量。例如，如果你想将形为（a，b，c）的数组v重塑为维度为(a*b, 3)的向量，则可以执行以下操作：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = v.reshape((v.shape[<span class="number">0</span>]*v.shape[<span class="number">1</span>], v.shape[<span class="number">2</span>])) <span class="comment"># v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c</span></span><br></pre></td></tr></table></figure><br>实现函数如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span>(<span class="params">image</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	iamge -- 一个(length, height, depth)类型的numpy数组</span></span><br><span class="line"><span class="string">	</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	v -- 一个(length*height*depth, 1)类型的向量</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	v = image.reshape(image.shape[<span class="number">0</span>] * image.shape[<span class="number">1</span>] * image.shape[<span class="number">2</span>], <span class="number">1</span>)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> v</span><br></pre></td></tr></table></figure></p>
<h4 id="行标准化"><a href="#行标准化" class="headerlink" title="行标准化"></a>行标准化</h4><p>我们在机器学习和深度学习中使用的另一种常见技术是对数据进行标准化。 由于归一化后梯度下降的收敛速度更快，通常会表现出更好的效果。 通过归一化，也就是将x更改为$ \frac{x}{| x|} $（将x的每个行向量除以其范数）。</p>
<p>以上为作业上内容，但个人还是无法理解归一化后为什么梯度下降的收敛更快。但有一点敢肯定的就是归一化后可以避免计算机计算结果得到大数而导致计算错误，比如算$e^{1000}$后计算机的结果为:<br><img src= "/img/loading.gif" data-lazy-src="/2020/10/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%801/e_1000.jpg"><br>从结果中可以看到计算机报错，如果将1000换成是分数，结果就能求出，这是由计算机本身的性能决定的。</p>
<p>求范数时得用到<a href="https://blog.csdn.net/hqh131360239/article/details/79061535">np.linalg.norm</a>函数。用法在链接中有详细介绍。</p>
<p><strong>练习</strong>：执行 normalizeRows（）来标准化矩阵的行。 将此函数应用于输入矩阵x之后，x的每一行应为单位长度（即长度为1）向量。<br>函数实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	功能：标准化举证的行</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	x -- 一个(n, m)类型的numpy矩阵</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	x -- 标准化后的numpy矩阵</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	x_norm = np.linalg.norm(x, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) <span class="comment">#求每行的范数</span></span><br><span class="line">	x = x / x_norm <span class="comment">#更新x矩阵</span></span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h4 id="广播和softmax函数"><a href="#广播和softmax函数" class="headerlink" title="广播和softmax函数"></a>广播和softmax函数</h4><p>在numpy中广播是十分重要的，实现了矩阵和数值之间和矩阵和矩阵之间的快速运算，详情见<a href="https://www.runoob.com/numpy/numpy-broadcast.html">numpy广播</a>。</p>
<p><strong>练习</strong>: 使用numpy实现softmax函数。 你可以将softmax理解为算法需要对两个或多个类进行分类时使用的标准化函数。<br><strong>Instructions</strong>:</p>
<ul>
<li><p>$ \text{for } x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}<br>  x_1  &amp;&amp;<br>  x_2 &amp;&amp;<br>  …  &amp;&amp;<br>  x_n<br>\end{bmatrix}) = \begin{bmatrix}<br>   \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &amp;&amp;<br>  \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &amp;&amp;<br>  …  &amp;&amp;<br>  \frac{e^{x_n}}{\sum_{j}e^{x_j}}<br>\end{bmatrix} $ </p>
</li>
<li><p>$\text{for a matrix } x \in \mathbb{R}^{m \times n} \text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  <script type="math/tex">softmax(x) = softmax\begin{bmatrix}
  x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
  x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  x_{m1} & x_{m2} & x_{m3} & \dots  & x_{mn}
\end{bmatrix} = \begin{bmatrix}
  \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} & \dots  & \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\
  \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} & \dots  & \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} & \dots  & \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}
\end{bmatrix} = \begin{pmatrix}
  softmax\text{(first row of x)}  \\
  softmax\text{(second row of x)} \\
  ...  \\
  softmax\text{(last row of x)} \\
\end{pmatrix}</script></p>
</li>
</ul>
<p>softmax实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	x -- 一个形状为(n, m)的numpy矩阵</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	s -- 一个形状为(n, m)的值为softmax(x)的numpy矩阵</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	x_exp = np.exp(x)</span><br><span class="line">	x_sum = np.sum(x_exp, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) <span class="comment">#对每行exp后的值求和</span></span><br><span class="line">	s = x_exp / x_sum</span><br><span class="line">	<span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><br>之前用pytorch写过手写数字识别的小程序，对于softmax函数映像很深。因为手写数字识别程序主要是通过训练来提高识别到真实数据的概率的方式来输出识别到的数字的。我目前认为softmax主要是将数值归一化来描述概率。</p>
<h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>这里介绍了几个实验，目的就是为了说明用矩阵运算处理数据要比单个数值处理的运算速度更加快，这是由计算机的性能决定的，具体是为什么还待日后研究。</p>
<h4 id="实现L1和L2损失函数"><a href="#实现L1和L2损失函数" class="headerlink" title="实现L1和L2损失函数"></a>实现L1和L2损失函数</h4><p>通俗来讲损失函数主要的功能其实就是求神经网络训练的值和真实值之间的差距，训练神经网络的目的其实就是缩小这个差距来使训练出来的值尽可能接近真实值。<br>L1损失函数公式：<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<script type="math/tex">\begin{align*} & L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align*}</script><br>代码实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span>(<span class="params">yhat, y</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	yhat -- 预测值</span></span><br><span class="line"><span class="string">	y -- 真实值</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	loss -- 损失函数求得的损失值</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	loss = np.sum(np.abs(y - yhat))</span><br><span class="line">	<span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><br>L2损失函数公式：<br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<script type="math/tex">\begin{align*} & L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}</script><br>代码实现如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span>(<span class="params">yhat, y</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	参数：</span></span><br><span class="line"><span class="string">	yhat -- 预测值</span></span><br><span class="line"><span class="string">	y -- 真实值</span></span><br><span class="line"><span class="string">	返回值：</span></span><br><span class="line"><span class="string">	loss -- 损失函数求得的损失值</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	loss = np.dot((y - yhat),(y - yhat).T)</span><br><span class="line">	<span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>吴恩达视频学习</category>
        <category>Lesson1</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>结构化机器学习</title>
    <url>/2020/10/15/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="机器学习策略"><a href="#机器学习策略" class="headerlink" title="机器学习策略"></a>机器学习策略</h2><p>机器学习策略：在优化深度学习系统时，通过一些分析机器学习问题的方法来制定一些策略，在优化过程中能够朝着有希望的过程中标前进。</p>
<h4 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h4><hr>
<p>正交化：如果两个方向成90<sup>o</sup>夹角，那么这两个方向正交。正交化其实就是把要解决的问题分成不同并互不影响的维度来解决。</p>
<p>举例：如果要调节一个视屏屏幕的大小，如果有一个按钮能同时调节屏幕的长和宽。那么这样在调节长的时候宽在变，在调节宽的时候长在变，这样就很难把屏幕调到目标大小。如果我们把一个按钮分成长和宽两个调节按钮，因为这两个按钮分别调节不同的维度，在调节长的时候宽不变，在调节宽的时候长不变，这样可以说这两个按钮是正交化了的，那么这样你就能够更准确地调节长和宽了。<img src= "/img/loading.gif" data-lazy-src="/2020/10/15/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Orthogonalization.jpeg" alt></p>
<h4 id="单一数字评估指标"><a href="#单一数字评估指标" class="headerlink" title="单一数字评估指标"></a>单一数字评估指标</h4><hr>
<p>个人理解：用一个数值化的标准来评估训练结果。</p>
<p>查准率(P)：在你的分类器标记为猫的例子中，有多少真的是猫。（假设有120张图片，分类器A说有100张图片是猫，事实真的是猫的只有95张图片，那么分类器A的查准率为95%）</p>
<p>查全率(R)：对于所有真猫的图片，你的分类器正确识别出了多少百分比。（假设有100张图片全是真的猫，然后分类器A却说只有90张是真的猫，那么分类器A的查全率为90%）</p>
<p>F1分数：$ \frac{2}{\frac{1}{P}+\frac{1}{R}} $<br>在数学中，这个函数叫做查准率和查全率的调和平均数。但非正式来说，你可以将它看成是某种查准率和查全率的平均值，只不过你算的不是直接的算术平均，而是用这个公式定义的调和平均。</p>
<h4 id="满足和优化指标"><a href="#满足和优化指标" class="headerlink" title="满足和优化指标"></a>满足和优化指标</h4><hr>
<p>满足指标：只要满足在阀值内就行的指标。（例如跑步比赛不能超过起跑线，那么距离起跑线多远都在规则内）</p>
<p>优化指标：指越优化性能越好的指标。（例如百米赛跑中当然是速度越快越好，那么跑步的速度就是优化指标）</p>
<h4 id="训练-开发-测试集划分"><a href="#训练-开发-测试集划分" class="headerlink" title="训练/开发/测试集划分"></a>训练/开发/测试集划分</h4><hr>
<p>三类数据集定义：</p>
<ul>
<li>training set：顾名思义，是用来训练模型的。因此它占了所有数据的绝大部分。</li>
<li>development set：用来对训练集训练出来的模型进行测试，通过测试结果来不断地优化模型。</li>
<li>test set：在训练结束后对训练出的模型进行一次最终的评估所用的数据集。</li>
</ul>
<p>在选择开发和训练集时最好能够对数据集均匀化，例如有A, B, C, D这来自四种不同地区的数据，如果开发集选择A, B测试集选择C, D的话，那么可能训练的系统不能够满足测试集的需求，所以最好把A, B, C, D这四类数据混合到一起再分配到开发集和测试集当中去。</p>
<h4 id="开发集和测试集的大小"><a href="#开发集和测试集的大小" class="headerlink" title="开发集和测试集的大小"></a>开发集和测试集的大小</h4><hr>
<p>大部分数据都集中在训练集上，除非你需要对最终投产系统有一个很精确的指标，所以开发集和测试集往往不需要很大。</p>
<h4 id="什么时候该改变开发-测试集和指标？"><a href="#什么时候该改变开发-测试集和指标？" class="headerlink" title="什么时候该改变开发/测试集和指标？"></a>什么时候该改变开发/测试集和指标？</h4><hr>
<p>分类错误率指标：$ Error = \frac{1}{m_{dev}}\sum_{i=1}^{m_{dev}}I\left \{ y_{pred}^{\left ( i \right )} \neq y^{\left ( i \right )}\right \} $<br>这个式子表示预测值与正确值不相等的个数占例子总数的比例。</p>
<p>加权重后指标：$ Error = \frac{1}{m_{dev}}\sum_{i=1}^{m_{dev}}w^{\left ( i \right )}I\left \{ y_{pred}^{\left ( i \right )} \neq y^{\left ( i \right )}\right \} $<br>在预测值不等于实际值时的前提下，预测值是特别不满意的数值，就在此预测结果上加上权重来表示自己想要的错误率。</p>
<p>归一化常数错误指标：$ Error = \frac{1}{\sum w^{\left ( i \right )}}\sum_{i=1}^{m_{dev}}w^{\left ( i \right )}I\left \{ y_{pred}^{\left ( i \right )} \neq y^{\left ( i \right )}\right \} $<br>此式子能够让所有错误率相加等于一，就是所谓的归一化。</p>
<h4 id="为什么是人的表现？"><a href="#为什么是人的表现？" class="headerlink" title="为什么是人的表现？"></a>为什么是人的表现？</h4><hr>
<p>算法发展过快已经能和人进行比较了。</p>
<h4 id="可避免偏差"><a href="#可避免偏差" class="headerlink" title="可避免偏差"></a>可避免偏差</h4><hr>
<p>贝叶斯错误率：指理论上错误率的上限。<br>可避免偏差：贝叶斯错误率或者对贝叶斯错误率的估计和训练错误率之间的差值。</p>
<p>当可避免偏差过大时说明可以优化的空间比较大，说明还可以继续做调整。</p>
<h4 id="理解人的表现"><a href="#理解人的表现" class="headerlink" title="理解人的表现"></a>理解人的表现</h4><hr>
<p>贝叶斯错误率一定是最优错误率，不管多少团队还是个人都无法比这个错误率更优。有时候可以用人的错误率来近似代替贝叶斯错误率来计算可避免偏差。</p>
<h4 id="超过人的表现"><a href="#超过人的表现" class="headerlink" title="超过人的表现"></a>超过人的表现</h4><hr>
<p>当计算机训练到接近人类水平或者超越人类水平的时候，以后的学习速度会越来越慢。</p>
<h4 id="改善你的模型的表现"><a href="#改善你的模型的表现" class="headerlink" title="改善你的模型的表现"></a>改善你的模型的表现</h4><hr>
<h4 id="进行错误分析"><a href="#进行错误分析" class="headerlink" title="进行错误分析"></a>进行错误分析</h4><hr>
<p>通过人工的观察错误样本并从错误样本并人工统计错误样本中导致错误的最大可能的原因然后系统的分析错误样本来对制定之后的优化策略。</p>
<h4 id="清除标注错误的数据"><a href="#清除标注错误的数据" class="headerlink" title="清除标注错误的数据"></a>清除标注错误的数据</h4><hr>
<p>标注错误：在数据集中经常会遇到图片与对应的标注不匹配的错误。<br>随机错误：如果标注的人员没注意不小心把标注错了，只要产生错误足够随机。<br>系统错误：比如标记本来猫对应着是1，但是标记人员认为狗对应着是1，然后一直给狗标记为1，这就是系统错误。</p>
<p>深度学习算法对随机误差很健壮，但对系统性的错误就没那么健壮了。如果是随机错误，数据集过大，实际的错误率就不会太高。但如果是系统错误，对结果的影响就很大了。最后通过分析标注错误所占错误率的比例来判断是否有必要修改标记错误。<img src= "/img/loading.gif" data-lazy-src="/2020/10/15/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/mark.png" alt></p>
<h4 id="快速搭建你的第一个系统，并进行迭代"><a href="#快速搭建你的第一个系统，并进行迭代" class="headerlink" title="快速搭建你的第一个系统，并进行迭代"></a>快速搭建你的第一个系统，并进行迭代</h4><hr>
<p>先不要想太多，快速搭建系统然后再慢慢优化。初始系统的全部意义在于，有一个学习过的系统，有一个训练过的系统，让你确定偏差方差的范围，就可以知道下一步应该优先做什么，让你能够进行错误分析，可以观察一些错误，然后想出所有能走的方向，哪些是实际上最有希望的方向。</p>
<h4 id="使用来自不同分布的数据，进行训练和测试"><a href="#使用来自不同分布的数据，进行训练和测试" class="headerlink" title="使用来自不同分布的数据，进行训练和测试"></a>使用来自不同分布的数据，进行训练和测试</h4><hr>
<p>当有一大部分是理想的数据集，还有一小部分是普通的数据集的时候，不应该把这两种数据集混合在一起均匀分配到每个集合中。应该将普通的数据当做把心才能更好的满足用户的需求。<img src= "/img/loading.gif" data-lazy-src="/2020/10/15/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/distributing.png" alt></p>
<h4 id="不匹配数据划分的偏差与方差"><a href="#不匹配数据划分的偏差与方差" class="headerlink" title="不匹配数据划分的偏差与方差"></a>不匹配数据划分的偏差与方差</h4><hr>
<p>先加了个train_dev的错误率，这部分的数据集是来自训练的数据集，如果train_dev的错误率与train的错误率差值过大，则说明是算法的问题，如果train_dev的错误率与dev的错误率差值过大说明是数据集的问题。<img src= "/img/loading.gif" data-lazy-src="/2020/10/15/%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/train_dev.png" alt></p>
<h4 id="定位数据不匹配"><a href="#定位数据不匹配" class="headerlink" title="定位数据不匹配"></a>定位数据不匹配</h4><hr>
<h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><hr>
<h4 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h4><hr>
<h4 id="什么是端对端的学习"><a href="#什么是端对端的学习" class="headerlink" title="什么是端对端的学习"></a>什么是端对端的学习</h4><hr>
<h4 id="是否要使用端对端的深度学习"><a href="#是否要使用端对端的深度学习" class="headerlink" title="是否要使用端对端的深度学习"></a>是否要使用端对端的深度学习</h4><hr>
]]></content>
      <tags>
        <tag>吴恩达视屏学习</tag>
        <tag>结构化机器学习</tag>
      </tags>
  </entry>
</search>
